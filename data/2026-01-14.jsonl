{"id": "2601.08370", "categories": ["q-bio.PE"], "pdf": "https://arxiv.org/pdf/2601.08370", "abs": "https://arxiv.org/abs/2601.08370", "authors": ["Mathieu Ardyna", "Marcel Nicolaus", "Marie-Noëlle Houssais", "Jean-Christophe Raut", "Hélène Angot", "Kelsey Bisson", "Kristina A Brown", "J Michel Flores", "Pierre E Galand", "Jean-François Ghiglione", "Kathy S Law", "François Ravetta", "Julia Schmale", "Jeroen E Sonke", "Marcel Babin", "Maxime Geoffroy", "Lars-Eric Heimbürger-Boavida", "Connie Lovejoy", "Søren Rysgaard", "Nina Schuback", "Martin Vancoppenolle", "Jean-Eric Tremblay", "Chris Bowler", "Lee Karp-Boss", "Romain Troublé"], "title": "Tara Polaris expeditions: Sustained decadal observations of the coupled Arctic system in rapid transition", "comment": null, "summary": "The coupled Arctic system is in rapid transition and is set to undergo further dramatic changes over the coming decades. These changes will lead most likely to an ice-free ocean in summer, expected before mid-century. The Arctic will become more strongly influenced by atmospheric and oceanographic processes characteristic of mid-latitudes, increasing the prevalence of contaminants and new biological species. This ongoing transition of the Arctic to a new state necessitates systematic monitoring of all sentinels (variables that make an essential contribution to characterizing the Earth's state) to improve our understanding of the system, enhance forecasting and support knowledge-based decisions. Here, we describe a sustained multi-decadal observation program to be implemented on the Tara Polar Station between 2026 and 2046. The monitoring program is designed as a series of year-long drift expeditions, called Tara Polaris, in the central Arctic Ocean, covering all seasons. The multidisciplinary data will bridge ecological, geochemical, biological, and physical parameters and processes in the atmosphere, sea ice and ocean. In addition, data collected with consistent methodologies over a 20-year period will make it possible to distinguish long-term trends from seasonal and interannual variability. In this paper, we discuss specific measurement challenges in each compartment (i.e., atmosphere, sea ice and ocean) along key sentinels and the most pressing scientific questions to be addressed. The expected outcomes of the Tara Polaris program will enable us to understand and quantify the main feedbacks of the coupled Arctic system, with their seasonal and interannual trends and spatial variability."}
{"id": "2601.08538", "categories": ["q-bio.PE", "math.PR"], "pdf": "https://arxiv.org/pdf/2601.08538", "abs": "https://arxiv.org/abs/2601.08538", "authors": ["Jonathan A Chetwynd-Diggle", "Bjarki Eldon"], "title": "Beta-coalescents when sample size is large", "comment": "84 pages; 4 figures", "summary": "Sweepstakes reproduction refers to a highly skewed individual recruitment success without involving natural selection and may apply to individuals in broadcast spawning populations characterised by Type III survivorship. We consider an extension of the model of sweepstakes reproduction for a haploid panmictic population of constant size $N$; the extension also works as an alternative to the Wright-Fisher model. Our model incorporates an upper bound on the random number of potential offspring (juveniles) produced by a given individual. Depending on how the bound behaves relative to the total population size, we obtain the Kingman coalescent, an incomplete Beta-coalescent, or the (complete) Beta-coalescent. We argue that applying such an upper bound is biologically reasonable. Moreover, we estimate the error of the coalescent approximation. The error estimates reveal that convergence can be slow, and small sample size can be sufficient to invalidate convergence, for example if the stated bound is of the form $N/\\log N$. We use simulations to investigate the effect of increasing sample size on the site-frequency spectrum. When the limit is a Beta-coalescent, the site frequency spectrum will be as predicted by the limiting tree even though the full coalescent tree may deviate from the limiting one. When in the domain of attraction of the Kingman coalescent the effect of increasing sample size depends on the effective population size as has been noted in the case of the Wright-Fisher model. Conditioning on the population ancestry (the random ancestral relations of the entire population at all times) may have little effect on the site-frequency spectrum for the models considered here (as evidenced by simulation results)."}
{"id": "2601.07863", "categories": ["q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.07863", "abs": "https://arxiv.org/abs/2601.07863", "authors": ["Arturo Tozzi"], "title": "From local defects to shear-organized biofilms in tonsillar crypts via computational simulations", "comment": "11 pates, 5 figures, 1 table", "summary": "Biofilms in human tonsillar crypts show long term persistence with episodic dispersal that current biochemical and microbiological descriptions do not fully explain, particularly with respect to spatial localization. We introduce a biophysical framework in which tonsillar biofilm dynamics arise from the interaction between two mechanical phenomena: a Kosterlitz Thouless type defect nucleation process and a Kelvin Helmholtz type shear driven interfacial instability. Crypt geometry is modeled as a confined, heterogeneous environment that promotes mechanically persistent surface defects generated by growth induced compression. Tangential shear associated with breathing and swallowing selectively amplifies these defects, producing organized surface deformations. Numerical simulations show that only the coexistence of both mechanisms yields localized, propagating, and persistent interface structures, whereas their absence leads to diffuse, unstructured dynamics."}
{"id": "2601.07871", "categories": ["q-bio.QM", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.07871", "abs": "https://arxiv.org/abs/2601.07871", "authors": ["Minh H. N. Le", "Tuan Vinh", "Thanh-Huy Nguyen", "Tao Li", "Bao Quang Gia Le", "Han H. Huynh", "Monika Raj", "Carl Yang", "Min Xu", "Nguyen Quoc Khanh Le"], "title": "Imaging-anchored Multiomics in Cardiovascular Disease: Integrating Cardiac Imaging, Bulk, Single-cell, and Spatial Transcriptomics", "comment": null, "summary": "Cardiovascular disease arises from interactions between inherited risk, molecular programmes, and tissue-scale remodelling that are observed clinically through imaging. Health systems now routinely generate large volumes of cardiac MRI, CT and echocardiography together with bulk, single-cell and spatial transcriptomics, yet these data are still analysed in separate pipelines. This review examines joint representations that link cardiac imaging phenotypes to transcriptomic and spatially resolved molecular states. An imaging-anchored perspective is adopted in which echocardiography, cardiac MRI and CT define a spatial phenotype of the heart, and bulk, single-cell and spatial transcriptomics provide cell-type- and location-specific molecular context. The biological and technical characteristics of these modalities are first summarised, and representation-learning strategies for each are outlined. Multimodal fusion approaches are reviewed, with emphasis on handling missing data, limited sample size, and batch effects. Finally, integrative pipelines for radiogenomics, spatial molecular alignment, and image-based prediction of gene expression are discussed, together with common failure modes, practical considerations, and open challenges. Spatial multiomics of human myocardium and atherosclerotic plaque, single-cell and spatial foundation models, and multimodal medical foundation models are collectively bringing imaging-anchored multiomics closer to large-scale cardiovascular translation."}
{"id": "2601.08147", "categories": ["q-bio.QM", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2601.08147", "abs": "https://arxiv.org/abs/2601.08147", "authors": ["Koyo Fujisaki", "Osei Horikoshi", "Yukitoshi Nagahara", "Kengo Morohashi"], "title": "Network Pharmacology Framework Characterizes Polypharmacological Properties of Dietary Flavonoids: Integration of Computational, Experimental, and Epidemiological Evidence", "comment": "117 pages, 8 figures, 3 supplementary tables (pages 34-117)", "summary": "Dietary flavonoids associate with disease prevention in epidemiological studies, yet their polypharmacological mechanisms remain unclear. We establish network pharmacology as a systematic framework to characterize flavonoid therapeutic properties through integrated computational, experimental, and epidemiological validation. We constructed a master network of 17,869 human proteins, 14 dietary flavonoids, and 1,496 FDA-approved drugs with 278,768 interactions. Flavonoids averaged 45.3 target proteins per compound compared to 16.8 for FDA-approved drugs (2.7-fold higher; p=7.5x10^-4), reflecting multi-target architecture. Statistical analysis revealed that 71.4% of flavonoids targeted proteins associated with cardiovascular drugs and 78.6% aligned with antineoplastic drug targets. MTT-based Jurkat cell assays confirmed network predictions: high-association flavonoids (luteolin LC50=31.4 microM, myricetin=29.5 microM) produced strong cytotoxicity, while low-association flavonoids showed minimal activity (LC50>200 microM). Network-predicted association strengths correlated with experimental bioactivity (Pearson r=0.918; R^2=0.843). We translated network associations into food-level predictions across 506 foods, identifying 685 food-drug therapeutic combinations. Systematic literature searches confirmed 96 associations supported by 132 unique references. Cardiovascular domains achieved 47.1% validation. Top-validated foods included tea (31 evidence items), blueberries (18 items), tomato (13 items), grape juice (10 items), and plum (9 items). Network pharmacology characterizes dietary polypharmacological properties and generates evidence-based food-therapeutic predictions, bridging nutritional science and systems pharmacology."}
{"id": "2601.08266", "categories": ["q-bio.QM", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2601.08266", "abs": "https://arxiv.org/abs/2601.08266", "authors": ["Ryan T. Black", "Steve A. Maas", "Wensi Wu", "Jalaj Maheshwari", "Tzanio Kolev", "Jeffrey A. Weiss", "Matthew A. Jolley"], "title": "An open-source computational framework for immersed fluid-structure interaction modeling using FEBio and MFEM", "comment": null, "summary": "Fluid-structure interaction (FSI) simulation of biological systems presents significant computational challenges, particularly for applications involving large structural deformations and contact mechanics, such as heart valve dynamics. Traditional ALE methods encounter fundamental difficulties with such problems due to mesh distortion, motivating immersed techniques. This work presents a novel open-source immersed FSI framework that strategically couples two mature finite element libraries: MFEM, a GPU-ready and scalable library with state-of-the-art parallel performance developed at Lawrence Livermore National Laboratory, and FEBio, a nonlinear finite element solver with sophisticated solid mechanics capabilities designed for biomechanics applications developed at the University of Utah. This coupling creates a unique synergy wherein the fluid solver leverages MFEM's distributed-memory parallelization and pathway to GPU acceleration, while the immersed solid exploits FEBio's comprehensive suite of hyperelastic and viscoelastic constitutive models and advanced solid mechanics modeling targeted for biomechanics applications. FSI coupling is achieved using a fictitious domain methodology with variational multiscale stabilization for enhanced accuracy on under-resolved grids expected with unfitted meshes used in immersed FSI. A fully implicit, monolithic scheme provides robust coupling for strongly coupled FSI characteristic of cardiovascular applications. The framework's modular architecture facilitates straightforward extension to additional physics and element technologies. Several test problems are considered to demonstrate the capabilities of the proposed framework, including a 3D semilunar heart valve simulation. This platform addresses a critical need for open-source immersed FSI software combining advanced biomechanics modeling with high-performance computing infrastructure."}
{"id": "2601.08318", "categories": ["q-bio.QM", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.08318", "abs": "https://arxiv.org/abs/2601.08318", "authors": ["Zhengye Pan", "Jianwei Zuo", "Jiajia Luo"], "title": "Disentangling History and Propagation Dependencies in Cross-Subject Knee Contact Stress Prediction Using a Shared MeshGraphNet Backbone", "comment": null, "summary": "Background:Subject-specific finite element analysis accurately characterizes knee joint mechanics but is computationally expensive. Deep surrogate models provide a rapid alternative, yet their generalization across subjects under limited pose and load inputs remains unclear. It remains unclear whether the dominant source of prediction uncertainty arises from temporal history dependence or spatial propagation dependence. Methods:To disentangle these factors, we employed a shared MGN backbone with a fixed mesh topology. A dataset of running trials from nine subjects was constructed using an OpenSim-FEBio workflow. We developed four model variants to isolate specific dependencies: (1) a baseline MGN; (2) CT-MGN, incorporating a Control Transformer to encode short-horizon history; (3) MsgModMGN, applying state-conditioned modulation to message passing for adaptive propagation; (4) CT-MsgModMGN, combining both mechanisms. Models were evaluated using a rigorous grouped 3-fold cross-validation on unseen subjects.Results:The models incorporating history encoding significantly outperformed the baseline MGN and MsgModMGN in global accuracy and spatial consistency. Crucially, the CT module effectively mitigated the peak-shaving defect common in deep surrogates, significantly reducing peak stress prediction errors. In contrast, the spatial propagation modulation alone yielded no significant improvement over the baseline, and combining it with CT provided no additional benefit.Conclusion:Temporal history dependence, rather than spatial propagation modulation, is the primary driver of prediction uncertainty in cross-subject knee contact mechanics. Explicitly encoding short-horizon driver sequences enables the surrogate model to recover implicit phase information, thereby achieving superior fidelity in peak-stress capture and high-risk localization compared to purely state-based approaches."}
{"id": "2601.08701", "categories": ["q-bio.QM", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.08701", "abs": "https://arxiv.org/abs/2601.08701", "authors": ["Tammar Truzman", "Matthew A. Lambon Ralph", "Ajay D. Halai"], "title": "Automated Lesion Segmentation of Stroke MRI Using nnU-Net: A Comprehensive External Validation Across Acute and Chronic Lesions", "comment": "32 pages, 7 figures. Submitted to Brain. Code and trained models available", "summary": "Accurate and generalisable segmentation of stroke lesions from magnetic resonance imaging (MRI) is essential for advancing clinical research, prognostic modelling, and personalised interventions. Although deep learning has improved automated lesion delineation, many existing models are optimised for narrow imaging contexts and generalise poorly to independent datasets, modalities, and stroke stages. Here, we systematically evaluated stroke lesion segmentation using the nnU-Net framework across multiple heterogeneous, publicly available MRI datasets spanning acute and chronic stroke. Models were trained and tested on diffusion-weighted imaging (DWI), fluid-attenuated inversion recovery (FLAIR), and T1-weighted MRI, and evaluated on independent datasets. Across stroke stages, models showed robust generalisation, with segmentation accuracy approaching reported inter-rater reliability. Performance varied with imaging modality and training data characteristics. In acute stroke, DWI-trained models consistently outperformed FLAIR-based models, with only modest gains from multimodal combinations. In chronic stroke, increasing training set size improved performance, with diminishing returns beyond several hundred cases. Lesion volume was a key determinant of accuracy: smaller lesions were harder to segment, and models trained on restricted volume ranges generalised poorly. MRI image quality further constrained generalisability: models trained on lower-quality scans transferred poorly, whereas those trained on higher-quality data generalised well to noisier images. Discrepancies between predictions and reference masks were often attributable to limitations in manual annotations. Together, these findings show that automated lesion segmentation can approach human-level performance while identifying key factors governing generalisability and informing the development of lesion segmentation tools."}
