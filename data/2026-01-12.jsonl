{"id": "2601.05367", "categories": ["q-bio.PE"], "pdf": "https://arxiv.org/pdf/2601.05367", "abs": "https://arxiv.org/abs/2601.05367", "authors": ["Parul Johri", "Fanny Pouyet", "Brian Charlesworth"], "title": "The rights and wrongs of rescaling in population genetics simulations", "comment": null, "summary": "Computer simulations of complex population genetic models are an essential tool for making sense of the large-scale datasets of multiple genome sequences from a single species that are becoming increasingly available. A widely used approach for reducing computing time is to simulate populations that are much smaller than the natural populations that they are intended to represent, by using parameters such as selection coefficients and mutation rates, whose products with the population size correspond to those of the natural populations. This approach has come to be known as rescaling, and is justified by the theory of the genetics of finite populations. Recently, however, there have been criticisms of this practice, which have brought to light situations in which it can lead to erroneous conclusions. This paper reviews the theoretical basis for rescaling, and relates it to current practice in population genetics simulations. It shows that some population genetic statistics are scaleable while others are not. Additionally, it shows that there are likely to be problems with rescaling when simulating large chromosomal regions, due to the non-linear relation between the physical distance between a pair of separate nucleotide sites and the frequency of recombination between them. Other difficulties with rescaling can arise in connection with simulations of selection on complex traits, and with populations that reproduce partly by self-fertilization or asexual reproduction. A number of recommendations are made for good practice in relation to rescaling."}
{"id": "2601.05531", "categories": ["q-bio.GN", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05531", "abs": "https://arxiv.org/abs/2601.05531", "authors": ["Eliatan Niktab", "Hardip Patel"], "title": "DNATokenizer: A GPU-First Byte-to-Identifier Tokenizer for High-Throughput DNA Language Models", "comment": null, "summary": "Tokenization sits at the boundary between high-throughput genomic input and GPU compute, posing challenges in both algorithm design and system throughput. Overlapping k-mer tokenization can introduce information leakage under masked language modeling (MLM) and may degrade downstream accuracy. Single-nucleotide tokenization avoids leakage and preserves per-base fidelity, but it greatly increases sequence length for attention-based architectures. Non-overlapping k-mers and byte-pair encoding (BPE) provide compression and avoid leakage, at the cost of boundary sensitivity or reduced interpretability. Empirically, the choice of tokenization interacts strongly with model architecture and task requirements. At the system level, however, standard string tokenizers and host-bound vocabulary lookups dominate wall-clock time once inputs reach billions of bases, regardless of the tokenization algorithm. We present DNATok, a high-performance, GPU-first tokenization system that replaces general-purpose string processing with byte lookup table (LUT)-based identifier streaming and an overlapped host-to-device (H2D)/compute pipeline using pinned memory and architectural parallelism. DNATok is vocabulary-agnostic: it accelerates single-nucleotide, non-overlapping k-mer, and BPE tokenization, and integrates as a drop-in systems layer beneath genomic foundation models. DNATok achieves 84-95x higher encoding throughput than optimized Hugging Face baselines and up to 1.9x higher H2D throughput. End-to-end streaming reaches 1.27-1.84e8 tokens/s depending on configuration, effectively removing tokenization as a bottleneck for production-scale training and inference."}
{"id": "2601.05605", "categories": ["q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.05605", "abs": "https://arxiv.org/abs/2601.05605", "authors": ["Yue Hu", "YingChao Liu"], "title": "AntibodyDesignBFN: High-Fidelity Fixed-Backbone Antibody Design via Discrete Bayesian Flow Networks", "comment": "4 pages, 1 table, 4 equations", "summary": "The computational design of antibodies with high specificity and affinity is a cornerstone of modern therapeutic development. While deep generative models, particularly Denoising Diffusion Probabilistic Models (DDPMs), have demonstrated the ability to generate realistic antibody structures, they often suffer from high computational costs and the difficulty of modeling discrete variables like amino acid sequences. In this work, we present AntibodyDesignBFN, a novel framework for fixed-backbone antibody design based on Discrete Bayesian Flow Networks(BFN). Unlike standard diffusion models that rely on Gaussian noise removal or complex discrete corruption processes, BFNs operate directly on the parameters of the data distribution, enabling a continuous-time, fully differentiable generative process on the probability simplex. While recent pioneering works like IgCraft and AbBFN have introduced BFNs to the domain of antibody sequence generation and inpainting, our work focuses specifically on the inverse folding task-designing sequences that fold into a fixed 3D backbone. By integrating a lightweight Geometric Transformer utilizing Invariant Point Attention (IPA) and a resource-efficient training strategy with gradient accumulation, our model achieves superior performance. Evaluations on a rigorous 2025 temporal test set reveal that AntibodyDesignBFN achieves a remarkable 48.1% Amino Acid Recovery (AAR) on H-CDR3, demonstrating that BFNs, when conditioned on 3D geometric constraints, offer a robust mathematical framework for high-fidelity antibody design.Code and model checkpoints are available at https://github.com/YueHuLab/AntibodyDesignBFN and https://huggingface.co/YueHuLab/AntibodyDesignBFN, respectively."}
{"id": "2601.05648", "categories": ["q-bio.GN", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.05648", "abs": "https://arxiv.org/abs/2601.05648", "authors": ["Haoran Wang", "Xuanyi Zhang", "Shuangsang Fang", "Longke Ran", "Ziqing Deng", "Yong Zhang", "Yuxiang Li", "Shaoshuai Li"], "title": "Open World Knowledge Aided Single-Cell Foundation Model with Robust Cross-Modal Cell-Language Pre-training", "comment": "41 pages", "summary": "Recent advancements in single-cell multi-omics, particularly RNA-seq, have provided profound insights into cellular heterogeneity and gene regulation. While pre-trained language model (PLM) paradigm based single-cell foundation models have shown promise, they remain constrained by insufficient integration of in-depth individual profiles and neglecting the influence of noise within multi-modal data. To address both issues, we propose an Open-world Language Knowledge-Aided Robust Single-Cell Foundation Model (OKR-CELL). It is built based on a cross-modal Cell-Language pre-training framework, which comprises two key innovations: (1) leveraging Large Language Models (LLMs) based workflow with retrieval-augmented generation (RAG) enriches cell textual descriptions using open-world knowledge; (2) devising a Cross-modal Robust Alignment (CRA) objective that incorporates sample reliability assessment, curriculum learning, and coupled momentum contrastive learning to strengthen the model's resistance to noisy data. After pretraining on 32M cell-text pairs, OKR-CELL obtains cutting-edge results across 6 evaluation tasks. Beyond standard benchmarks such as cell clustering, cell-type annotation, batch-effect correction, and few-shot annotation, the model also demonstrates superior performance in broader multi-modal applications, including zero-shot cell-type annotation and bidirectional cell-text retrieval."}
{"id": "2601.05921", "categories": ["q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.05921", "abs": "https://arxiv.org/abs/2601.05921", "authors": ["Philip Gerlee", "Torbjörn Lundh", "Anna Saxne Jöud", "Henrik Thorén"], "title": "Evaluating infectious disease forecasts in a cost-loss situation", "comment": null, "summary": "In order for epidemiological forecasts to be useful for decision-makers the forecasts need to be properly validated and evaluated. Although several metrics fore evaluation have been proposed and used none of them account for the potential costs and losses that the decision-maker faces. We have adapted a decision-theoretic framework to an epidemiological context which assigns a Value Score (VS) to each model by comparing the expected expense of the decision-maker when acting on the model forecast to the expected expense obtained from acting on historical event probabilities. The VS depends on the cost-loss ratio and a positive VS implies added value for the decision-maker whereas a negative VS means that historical event probabilities outperform the model forecasts. We apply this framework to a subset of model forecasts of influenza peak intensity from the FluSight Challenge and show that most models exhibit a positive VS for some range of cost-loss ratios. However, there is no clear relationship between the VS and the original ranking of the model forecasts obtained using a modified log score. This is in part explained by the fact that the VS is sensitive to over- vs. under-prediction, which is not the case for standard evaluation metrics. We believe that this type of context-sensitive evaluation will lead to improved utilisation of epidemiological forecasts by decision-makers."}
